{"cells":[{"cell_type":"code","execution_count":null,"id":"6dcaa731","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":943,"status":"ok","timestamp":1660178860716,"user":{"displayName":"Martina De Luca Barcello","userId":"10697530983672350098"},"user_tz":240},"id":"6dcaa731","outputId":"61db096c-7349-4bf6-8569-d7f4ac7112e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Set up drive and colab for reading files from shared drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"a9f3da82","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11704,"status":"ok","timestamp":1660178872417,"user":{"displayName":"Martina De Luca Barcello","userId":"10697530983672350098"},"user_tz":240},"id":"a9f3da82","outputId":"f1eccb19-a094-4042-97b8-95b7e7e776da"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.7.1)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.21.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.97)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.21.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.12.0+cu113)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n","Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.8.1)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.13.0+cu113)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.3)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.7.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.12.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.25.11)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"]}],"source":["# Imports and installation of additional packages\n","\n","import numpy as np\n","from numpy import array\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import string\n","import os\n","import glob\n","\n","from tensorflow.keras import Input, layers, optimizers\n","from tensorflow.keras.preprocessing import sequence, image\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout, add\n","from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.utils import to_categorical\n","from nltk.translate.bleu_score import sentence_bleu\n","from tensorflow.keras.utils import plot_model\n","\n","\n","from random import randint\n","!pip install datasets\n","from datasets import load_metric\n","import nltk\n","from nltk import word_tokenize\n","from nltk.translate.bleu_score import corpus_bleu\n","\n","from PIL import Image\n","\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","!pip install transformers\n","!pip install sentence-transformers\n","\n","from sentence_transformers import SentenceTransformer\n","from tensorflow.keras.metrics import CosineSimilarity\n","\n","from tqdm import tqdm\n","import pickle"]},{"cell_type":"code","execution_count":null,"id":"66983f31","metadata":{"id":"66983f31"},"outputs":[],"source":["#Variables for file paths\n","glove_path = '/content/drive/Shareddrives/612 Project Shared Drive/glove6b'\n","\n","# File paths for Flickr8k dataset\n","file_image_feature_train = '/content/drive/Shareddrives/612 Project Shared Drive/image_features_train_8k.pkl'\n","file_image_feature_test = '/content/drive/Shareddrives/612 Project Shared Drive/image_features_test_8k.pkl'\n","file_model = '/content/drive/Shareddrives/612 Project Shared Drive/model_Flickr8k.pkl' # save the model to pickle\n","token_path = \"/content/drive/Shareddrives/612 Project Shared Drive/Flickr8k_text/Flickr8k.token.txt\" # Contains descriptions for image id\n","train_images_path = '/content/drive/Shareddrives/612 Project Shared Drive/Flickr8k_text/Flickr_8k.trainImages.txt' # file with train images ids\n","test_images_path = '/content/drive/Shareddrives/612 Project Shared Drive/Flickr8k_text/Flickr_8k.testImages.txt' # file with test images ids\n","images_path = '/content/drive/Shareddrives/612 Project Shared Drive/Flicker8k_Dataset/' # folder with all images\n","\n","# File paths for Flickr30k dataset\n","#file_image_feature_train = '/content/drive/Shareddrives/612 Project Shared Drive/image_features_train_30k.pkl'\n","#file_image_feature_test = '/content/drive/Shareddrives/612 Project Shared Drive/image_features_test_30k.pkl'\n","#file_model = '/content/drive/Shareddrives/612 Project Shared Drive/model_Flickr30k.pkl'\n","#token_path = \"/content/drive/Shareddrives/612 Project Shared Drive/Flickr30k_text/Flickr30k.token.txt\" # Contains descriptions for image id\n","#train_images_path = '/content/drive/Shareddrives/612 Project Shared Drive/Flickr30k_text/Flickr_30k.trainImages.txt' # file with train images ids\n","#test_images_path = '/content/drive/Shareddrives/612 Project Shared Drive/Flickr30k_text/Flickr_30k.testImages.txt' # file with test images ids\n","#images_path = '/content/drive/Shareddrives/612 Project Shared Drive/Flicker30k_Dataset/' # folder with all images"]},{"cell_type":"code","execution_count":null,"id":"a0e1ddfa","metadata":{"id":"a0e1ddfa"},"outputs":[],"source":["# Get dataset for images and ids prepared\n","\n","def get_images_id(train_images_path):\n","    # load all the training image ids in a variable train from the trainImages.txt file\n","    doc = open(train_images_path,'r').read()\n","    dataset = list()\n","    for line in doc.split('\\n'):\n","        if len(line) > 1:\n","            identifier = line.split('.')[0]\n","            dataset.append(identifier)\n","    return set(dataset)\n","\n","def get_images_paths(path, images_path, img):\n","    # save all the training and testing image path name in train_img and test_img lists respectively\n","    t_images = set(open(path, 'r').read().strip().split('\\n'))\n","    t_img = []\n","    for i in img: \n","        if i[len(images_path):] in t_images:\n","            t_img.append(i)\n","    return t_img\n","\n","train = get_images_id(train_images_path) # set of train images id\n","test = get_images_id(test_images_path) # set of test images id\n","\n","img = glob.glob(images_path + '*.jpg')\n","train_img = get_images_paths(train_images_path, images_path, img) # list of path of train images\n","test_img = get_images_paths(test_images_path, images_path, img) # list of path of test images"]},{"cell_type":"code","execution_count":null,"id":"3c7dd501","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5226,"status":"ok","timestamp":1660178906764,"user":{"displayName":"Martina De Luca Barcello","userId":"10697530983672350098"},"user_tz":240},"id":"3c7dd501","outputId":"36c0df93-e935-4a58-a454-f715172fdfb2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Raw descriptions:  ['Two young guys with shaggy hair look at their hands while hanging out in the yard .', 'Two young , White males are outside near many bushes .', 'Two men in green shirts are standing in a yard .', 'A man in a blue shirt standing in a garden .', 'Two friends enjoy time spent together .']\n","Cleaned descriptions:  ['two young guys with shaggy hair look at their hands while hanging out in the yard', 'two young white males are outside near many bushes', 'two men in green shirts are standing in yard', 'man in blue shirt standing in garden', 'two friends enjoy time spent together']\n","Formatted descriptions:  1000092795 two young guys with shaggy hair look at their hands while hanging out in the yard\n","1000092795 two young white males are outside near many bushes\n","1000092795 two men in green shirts are standing in yard\n","1000092795 man in blue shirt standing in garden\n","1000092795 two friends enjoy time spent together\n","10002456 several men in hard hats are op\n","Tokenized train descriptions:  ['startseq two young guys with shaggy hair look at their hands while hanging out in the yard endseq', 'startseq two young white males are outside near many bushes endseq', 'startseq two men in green shirts are standing in yard endseq', 'startseq man in blue shirt standing in garden endseq', 'startseq two friends enjoy time spent together endseq']\n","Description Length: 35\n","List of captions:  ['startseq two young guys with shaggy hair look at their hands while hanging out in the yard endseq', 'startseq two young white males are outside near many bushes endseq', 'startseq two men in green shirts are standing in yard endseq', 'startseq man in blue shirt standing in garden endseq', 'startseq two friends enjoy time spent together endseq']\n"]}],"source":["# Clean train descriptions\n","\n","def import_descriptions(token_path):\n","    # Open file with descriptions for image id\n","    doc = open(token_path,'r').read()\n","    # Create dictionary with image_id as key and the list of captions as value\n","    descriptions = dict()\n","    for line in doc.split('\\n'):\n","        tokens = line.split()\n","        if len(line) > 2:\n","            image_id = tokens[0].split('.')[0]\n","            image_desc = ' '.join(tokens[1:])\n","            if image_id not in descriptions:\n","                descriptions[image_id] = list()\n","            descriptions[image_id].append(image_desc)\n","    return descriptions\n","\n","def clean_descriptions(descriptions):\n","    table = str.maketrans('', '', string.punctuation)\n","    for key, desc_list in descriptions.items():\n","        for i in range(len(desc_list)):\n","            desc = desc_list[i]\n","            desc = desc.split()\n","            desc = [word.lower() for word in desc] # lowercase\n","            desc = [w.translate(table) for w in desc] # remove punctuation\n","            desc = [word for word in desc if len(word)>1] # remove hanging 's' and 'a'\n","            desc = [word for word in desc if word.isalpha()] # remove tokens with numbers in them\n","            joint = ' '.join(desc)\n","            desc_list[i] =  joint.strip() # remove trailing spaces\n","    return descriptions\n","    \n","def format_descriptions(descriptions):\n","    # save the image ids and their new cleaned captions in the same format as the token.txt file\n","    lines = list()\n","    for key, desc_list in descriptions.items():\n","        for desc in desc_list:\n","            lines.append(key + ' ' + desc)\n","    new_descriptions = '\\n'.join(lines)\n","    return new_descriptions\n","\n","def add_tokens(descriptions, set_ids):\n","    # load the descriptions of the training images into a dictionary\n","    # adding two tokens in every caption, which are ‘startseq’ and ‘endseq’\n","    train_descriptions = dict()\n","    for line in descriptions.split('\\n'):\n","        tokens = line.split()\n","        image_id, image_desc = tokens[0], tokens[1:]\n","        if image_id in set_ids:\n","            if image_id not in train_descriptions:\n","                train_descriptions[image_id] = list()\n","            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n","            train_descriptions[image_id].append(desc)\n","    return train_descriptions\n","\n","def get_captions_from_dict(dictionary):\n","    # Create a list of all the training captions\n","    list_captions = []\n","    for key, val in dictionary.items():\n","        for cap in val:\n","            list_captions.append(cap)\n","    return list_captions\n","\n","def get_max_len(descriptions):\n","    # find out what the max length of a caption can be since we cannot have captions of arbitrary length\n","    # if caption max length too large, it is set as 35 maximum\n","    all_desc = list()\n","    for key in descriptions.keys():\n","        [all_desc.append(d) for d in descriptions[key]]\n","    lines = all_desc\n","    result = max(len(d.split()) for d in lines)\n","    if (result > 35):\n","        return 35\n","    else:\n","        return result\n","\n","descriptions = import_descriptions(token_path)\n","print('Raw descriptions: ', descriptions['1000268201_693b08cb0e']) # Flickr30k: 1000092795 - Flickr8k: 1000268201_693b08cb0e\n","cleaned_descriptions = clean_descriptions(descriptions)\n","print('Cleaned descriptions: ', cleaned_descriptions['1000268201_693b08cb0e'])  # Flickr30k: 1000092795 - Flickr8k: 1000268201_693b08cb0e\n","formatted_descriptions = format_descriptions(cleaned_descriptions)\n","print('Formatted descriptions: ', formatted_descriptions[0:348])\n","tokenized_train_descriptions = add_tokens(formatted_descriptions, train)\n","print('Tokenized train descriptions: ', tokenized_train_descriptions['1000268201_693b08cb0e'])  # Flickr30k: 1000092795 - Flickr8k: 1000268201_693b08cb0e\n","max_length = get_max_len(tokenized_train_descriptions)\n","print('Description Length: %d' % max_length)\n","all_train_captions = get_captions_from_dict(tokenized_train_descriptions)\n","print('List of captions: ', all_train_captions[0:5])"]},{"cell_type":"code","execution_count":null,"id":"5a222c45","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2282,"status":"ok","timestamp":1660178909043,"user":{"displayName":"Martina De Luca Barcello","userId":"10697530983672350098"},"user_tz":240},"id":"5a222c45","outputId":"2ef2f5f8-b537-441c-b176-3de72820b68e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original Vocabulary Size: 19712\n","Cleaned Vocabulary = 4522\n"]}],"source":["# Create vocabulary\n","\n","def create_vocab(descriptions):\n","    # create vocabulary of all the unique words in the captions\n","    vocabulary = set()\n","    for key in descriptions.keys():\n","            [vocabulary.update(d.split()) for d in descriptions[key]]\n","    return vocabulary\n","\n","def clean_vocab(word_count_threshold, captions):\n","    # reduce our vocabulary to only those words which occur at least 10 times in the entire corpus\n","    word_counts = {}\n","    nsents = 0\n","    for sent in captions:\n","        nsents += 1\n","        for w in sent.split(' '):\n","            word_counts[w] = word_counts.get(w, 0) + 1\n","    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n","    return vocab\n","\n","vocabulary = create_vocab(cleaned_descriptions)\n","print('Original Vocabulary Size: %d' % len(vocabulary))\n","cleaned_vocab = clean_vocab(10, all_train_captions)\n","print('Cleaned Vocabulary = %d' % (len(cleaned_vocab)))"]},{"cell_type":"code","execution_count":null,"id":"4f6282f4","metadata":{"id":"4f6282f4"},"outputs":[],"source":["# Create mappings between words and index\n","ixtoword = {}\n","wordtoix = {}\n","ix = 1\n","for w in cleaned_vocab:\n","    wordtoix[w] = ix\n","    ixtoword[ix] = w\n","    ix += 1\n","\n","vocab_size = len(ixtoword) + 1\n","\n","# Use GloVe embeddings to map the words to vectors\n","embeddings_index = {} \n","f = open(os.path.join(glove_path, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","    \n","# Create embedding matrix\n","embedding_dim = 200\n","embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","for word, i in wordtoix.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector"]},{"cell_type":"code","execution_count":null,"id":"9fd43fcf","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":514},"executionInfo":{"elapsed":351,"status":"error","timestamp":1660189708091,"user":{"displayName":"Martina De Luca Barcello","userId":"10697530983672350098"},"user_tz":240},"id":"9fd43fcf","outputId":"c023aa72-4fb6-4c33-fa20-8dacda7d8335"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-8b5b604cd15a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"# Extract features from training images\\n\\n# transfer learning using InceptionResNetV2 network which is pre-trained on the ImageNet dataset\\nmodel = InceptionResNetV2(weights='imagenet')\\n\\n# remove the softmax layer from the pre-trained model, since we are not doing the classification part\\nmodel_new = Model(model.input, model.layers[-2].output)\\n\\n# preprocess function to reshape the images to (299 x 299)\\ndef preprocess(image_path):\\n    img = image.load_img(image_path, target_size=(299, 299))\\n    x = image.img_to_array(img)\\n    x = np.expand_dims(x, axis=0)\\n    x = preprocess_input(x)\\n    return x\\n\\n# encode training and testing images using the model defined above\\ndef encode(image):\\n    image = preprocess(image) \\n    fea_vec = model_new.predict(image) \\n    fea_vec = np.reshape(fea_vec, fea_vec.shape[1])\\n    return fea_vec\\n\\nencoding_train = {}\\nfor img in tqdm(train_img):\\n    encoding_train[img[len(images_path):]] = encode(img)\\ntrain_features = encoding_train\\n\\nencoding_test = {}\\nfor img in tqdm(test_img):\\n    encoding_test[img[len(images_path):]] = encode(img)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'InceptionResNetV2' is not defined"]}],"source":["%%time\n","# Extract features from training images\n","\n","# transfer learning using InceptionResNetV2 network which is pre-trained on the ImageNet dataset\n","model = InceptionResNetV2(weights='imagenet')\n","\n","# remove the softmax layer from the pre-trained model, since we are not doing the classification part\n","model_new = Model(model.input, model.layers[-2].output)\n","\n","# preprocess function to reshape the images to (299 x 299)\n","def preprocess(image_path):\n","    img = image.load_img(image_path, target_size=(299, 299))\n","    x = image.img_to_array(img)\n","    x = np.expand_dims(x, axis=0)\n","    x = preprocess_input(x)\n","    return x\n","\n","# encode training and testing images using the model defined above\n","def encode(image):\n","    image = preprocess(image) \n","    fea_vec = model_new.predict(image) \n","    fea_vec = np.reshape(fea_vec, fea_vec.shape[1])\n","    return fea_vec\n","\n","encoding_train = {}\n","for img in tqdm(train_img):\n","    encoding_train[img[len(images_path):]] = encode(img)\n","train_features = encoding_train\n","\n","encoding_test = {}\n","for img in tqdm(test_img):\n","    encoding_test[img[len(images_path):]] = encode(img)"]},{"cell_type":"code","source":["# Save image features to file\n","pickle.dump(encoding_train, open(file_image_feature_train, 'wb')) \n","pickle.dump(encoding_test, open(file_image_feature_test, 'wb'))\n","\n","# Load image features from file\n","# encoding_train = pickle.load(open(file_image_feature_train, 'rb'))\n","# train_features = encoding_train\n","# encoding_test = pickle.load(open(file_image_feature_test, 'rb'))"],"metadata":{"id":"IOWKgZuRZlT2"},"id":"IOWKgZuRZlT2","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"bfa1ac41","metadata":{"id":"bfa1ac41"},"outputs":[],"source":["# Create model\n","# First part of the model is the Image Model, which takes the image features as input\n","inputs1 = Input(shape=(1536,))\n","fe1 = Dropout(0.5)(inputs1)\n","fe2 = Dense(256, activation='relu')(fe1)\n","fe3 = Dropout(0.5)(fe2)\n","fe4 = Dense(256, activation='relu')(fe3)\n","\n","# Second part of the model is the Language Model, which takes the embedded vectors of the captions as input\n","inputs2 = Input(shape=(max_length,))\n","se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n","se2 = Dropout(0.5)(se1)\n","se3 = LSTM(256,  return_sequences=True)(se2)\n","se4 = Dropout(0.5)(se3)\n","se5 = LSTM(256)(se4)\n","\n","# Final part of the model merges outputs of the two previous parts and generates final output of the model for predictions\n","decoder1 = add([fe4, se5])\n","decoder2 = Dense(256, activation='relu')(decoder1)\n","outputs = Dense(vocab_size, activation='softmax')(decoder2)\n","\n","model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","model.summary()\n","plot_model(model, show_shapes=True)"]},{"cell_type":"code","execution_count":null,"id":"01bd1a41","metadata":{"id":"01bd1a41"},"outputs":[],"source":["# Model Training Set Up\n","model.layers[2].set_weights([embedding_matrix])\n","model.layers[2].trainable = False\n","model.compile(loss='categorical_crossentropy', optimizer='adam')"]},{"cell_type":"code","execution_count":null,"id":"1ea6bcd7","metadata":{"id":"1ea6bcd7"},"outputs":[],"source":["# Function that prepares final training data for training the model\n","def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n","    X1, X2, y = list(), list(), list()\n","    n=0\n","    # loop for ever over images\n","    while 1:\n","        for key, desc_list in descriptions.items():\n","            n+=1\n","            # retrieve the photo feature\n","            photo = photos[key+'.jpg']\n","            for desc in desc_list:\n","                # encode the sequence\n","                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n","                # split one sequence into multiple X, y pairs\n","                for i in range(1, len(seq)):\n","                    # split into input and output pair\n","                    in_seq, out_seq = seq[:i], seq[i]\n","                    # pad input sequence\n","                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n","                    # encode output sequence\n","                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","                    # store\n","                    X1.append(photo)\n","                    X2.append(in_seq)\n","                    y.append(out_seq)\n","\n","            if n==num_photos_per_batch:\n","                yield ([array(X1), array(X2)], array(y))\n","                X1, X2, y = list(), list(), list()\n","                n=0"]},{"cell_type":"code","execution_count":null,"id":"545b6ed3","metadata":{"id":"545b6ed3"},"outputs":[],"source":["%%time\n","# Note: This cell can be omitted during excecution and load the trained model from the pkl file in the next cell\n","# Model Training \n","epochs = 100\n","batch_size = 32\n","steps = len(tokenized_train_descriptions)//batch_size\n","\n","generator = data_generator(tokenized_train_descriptions, train_features, wordtoix, max_length, batch_size)\n","model.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1)"]},{"cell_type":"code","source":["# Save model\n","#pickle.dump(model, open(file_model, 'wb'))\n","# Load model\n","model = pickle.load(open(file_model, 'rb'))"],"metadata":{"id":"aCrf66HHGABT"},"id":"aCrf66HHGABT","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"7070281e","metadata":{"id":"7070281e"},"outputs":[],"source":["# Define functions for model evaluation\n","\n","# Two methods for the caption generation are defined: greedy and beam search\n","def generate_caption_greedy(model, photo, max_length):\n","    in_text = 'startseq'\n","    for i in range(max_length):\n","        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n","        sequence = pad_sequences([sequence], maxlen=max_length)\n","        yhat = model.predict([photo,sequence], verbose=0)\n","        yhat = np.argmax(yhat)\n","        word = ixtoword[yhat]\n","        in_text += ' ' + word\n","        if word == 'endseq':\n","            break\n","\n","    final = in_text.split()\n","    final = final[1:-1]\n","    final = ' '.join(final)\n","    return final\n","\n","def generate_caption_beam(model, image, max_length, beam_index = 3):\n","    start = [wordtoix[\"startseq\"]]\n","    start_word = [[start, 0.0]]\n","    while len(start_word[0][0]) < max_length:\n","        temp = []\n","        for s in start_word:\n","            par_caps = sequence.pad_sequences([s[0]], maxlen=max_length, padding='post')\n","            preds = model.predict([image,par_caps], verbose=0)\n","            word_preds = np.argsort(preds[0])[-beam_index:]\n","            # Getting the top <beam_index>(n) predictions and creating a \n","            # new list so as to put them via the model again\n","            for w in word_preds:\n","                next_cap, prob = s[0][:], s[1]\n","                next_cap.append(w)\n","                prob += preds[0][w]\n","                temp.append([next_cap, prob])\n","                    \n","        start_word = temp\n","        # Sorting according to the probabilities\n","        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n","        # Getting the top words\n","        start_word = start_word[-beam_index:]\n","    \n","    start_word = start_word[-1][0]\n","    intermediate_caption = [ixtoword[i] for i in start_word]\n","    final_caption = []\n","    \n","    for i in intermediate_caption:\n","        if i != 'endseq':\n","            final_caption.append(i)\n","        else:\n","            break\n","\n","    final_caption = ' '.join(final_caption[1:])\n","    return final_caption\n","\n","# Function to generate captions with greedy method and calculate metrics for dataset of images\n","def evaluate_model(model, descriptions, photos, max_length):\n","    actual, predicted = list(), list()\n","    # step over the whole set\n","    for key, desc_list in tqdm(descriptions.items()):\n","        # generate description\n","        yhat = generate_caption_greedy(model, photos[key+'.jpg'].reshape((1,1536)), max_length)\n","        # store actual and predicted\n","        references = [d.split() for d in desc_list]\n","        actual.append(references)\n","        predicted.append(yhat.split())\n","    # calculate BLEU score\n","    print('Bleu Cumulative 1-gram: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n","    print('Bleu Cumulative 2-gram: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n","    print('Bleu Cumulative 3-gram: %f' % corpus_bleu(actual, predicted, weights=(0.33, 0.33, 0.33, 0)))\n","    print('Bleu Cumulative 4-gram: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n","    #print('Meteor: ', round(nltk.translate.meteor_score.meteor_score(actual, predicted),4))\n","    \n","# Function that runs all metrics for one example image based on candidate and reference captions\n","def run_all_metrics(reference, candidate):\n","    #print('Bleu Individual 1-gram: %f' % sentence_bleu(reference, candidate.split(), weights=(1, 0, 0, 0)))\n","    #print('Bleu Individual 2-gram: %f' % sentence_bleu(reference, candidate.split(), weights=(0, 1, 0, 0)))\n","    #print('Bleu Individual 3-gram: %f' % sentence_bleu(reference, candidate.split(), weights=(0, 0, 1, 0)))\n","    #print('Bleu Individual 4-gram: %f' % sentence_bleu(reference, candidate.split(), weights=(0, 0, 0, 1)))\n","    print('Bleu Cumulative 1-gram: %f' % sentence_bleu(reference, candidate.split(), weights=(1, 0, 0, 0)))\n","    print('Bleu Cumulative 2-gram: %f' % sentence_bleu(reference, candidate.split(), weights=(0.5, 0.5, 0, 0)))\n","    print('Bleu Cumulative 3-gram: %f' % sentence_bleu(reference, candidate.split(), weights=(0.33, 0.33, 0.33, 0)))\n","    print('Bleu Cumulative 4-gram: %f' % sentence_bleu(reference, candidate.split(), weights=(0.25, 0.25, 0.25, 0.25)))\n","    print('Meteor: ', round(nltk.translate.meteor_score.meteor_score(reference, candidate.split()),4))\n","\n","# Cosine similarity evaluation on test set - Set up\n","transformer_model = SentenceTransformer('stsb-roberta-large')\n","cosine_metric = CosineSimilarity(axis = -1)\n","\n","# Define auxilary functions for cosine similarity\n","def get_predictions(model, descriptions, photos, max_length, images_path = images_path):\n","    predictions = list()\n","    references = list()\n","    for key, desc_list in tqdm(descriptions.items()):\n","        form_descs = []\n","        # generate description\n","        yhat = generate_caption_greedy(model, photos[key+'.jpg'].reshape((1,1536)), max_length)\n","        form_desc = desc_list[0].split()[1:-1]\n","        form_desc = \" \".join(form_desc)\n","        # store predictions\n","        predictions.append(yhat)\n","        # store references\n","        references.append(form_desc)\n","    return (predictions, references)\n","\n","def get_cosine_similarity(model, predictions, references): \n","    cum_score = 0\n","    num_preds = len(predictions)\n","    \n","    encoded_preds = model.encode(predictions, convert_to_tensor=True).cpu()\n","    encoded_refs = model.encode(references, convert_to_tensor = True).cpu()\n","\n","    cosine_metric.reset_state()  \n","    cosine_metric.update_state(encoded_preds, encoded_refs)\n","  \n","    score = cosine_metric.result().numpy\n","    return score"]},{"cell_type":"code","execution_count":null,"id":"fb9e8cb4","metadata":{"id":"fb9e8cb4"},"outputs":[],"source":["# Trained model will be evaluated on training images, test images and random individual images to review results \n","# Get training set results and calculate metrics\n","evaluate_model(model, tokenized_train_descriptions, encoding_train, max_length)"]},{"cell_type":"code","execution_count":null,"id":"f63c6204","metadata":{"id":"f63c6204"},"outputs":[],"source":["# Test model on random image from test set\n","# generate random id for choosing test photo\n","pic = list(encoding_test.keys())[randint(0, len(encoding_test))] # random photo\n","# Specific photos to check:\n","#pic = '2148916767_644ea6a7fa.jpg' # Flickr30k: 2148916767 - Flickr8k: 2148916767_644ea6a7fa\n","#pic = '801607443_f15956d1ce.jpg' # Flickr30k: 801607443 - Flickr8k: 801607443_f15956d1ce\n","#pic = '3558370311_5734a15890.jpg' # Flickr30k: 3558370311 - Flickr8k: 3558370311_5734a15890\n","#pic = '2308978137_bfe776d541.jpg' # Flickr30k: 2308978137 - Flickr8k: 2308978137_bfe776d541\n","\n","# get image\n","image_random = encoding_test[pic].reshape((1,1536))\n","# print image\n","x=plt.imread(images_path+pic)\n","plt.imshow(x)\n","plt.axis('off')\n","plt.show()\n","\n","# compute prediction of caption\n","caption_greedy = generate_caption_greedy(model, image_random, max_length)\n","print('Generated Greedy Caption: ', caption_greedy)\n","caption_beam_3 = generate_caption_beam(model, image_random, max_length, 3)\n","print('Generated Beam K = 3 Caption: ', caption_beam_3)\n","caption_beam_5 = generate_caption_beam(model, image_random, max_length, 5)\n","print('Generated Beam K = 5 Caption: ', caption_beam_5)\n","caption_beam_7 = generate_caption_beam(model, image_random, max_length, 7)\n","print('Generated Beam K = 7 Caption: ', caption_beam_7)\n","caption_beam_10 = generate_caption_beam(model, image_random, max_length, 10)\n","print('Generated Beam K = 10 Caption: ', caption_beam_10)\n","\n","# Calculate metrics for sample random test image\n","reference = [\n","        descriptions[pic[:-4]][0].split(),\n","        descriptions[pic[:-4]][1].split(),\n","        descriptions[pic[:-4]][2].split(),\n","        descriptions[pic[:-4]][3].split(),\n","        descriptions[pic[:-4]][4].split()\n","]\n","print('---------------------------------------')\n","print('References for this image:', reference)\n","print('---------------------------------------')\n","print('Results for greedy prediction: ')\n","print('Candidate:', caption_greedy)\n","run_all_metrics(reference, caption_greedy)\n","print('---------------------------------------')\n","print('Results for beam 3 prediction: ')\n","print('Candidate:', caption_beam_3)\n","run_all_metrics(reference, caption_beam_3)\n","print('---------------------------------------')\n","print('Results for beam 5 prediction: ')\n","print('Candidate:', caption_beam_5)\n","run_all_metrics(reference, caption_beam_5)\n","print('---------------------------------------')\n","print('Results for beam 7 prediction: ')\n","print('Candidate:', caption_beam_7)\n","run_all_metrics(reference, caption_beam_7)\n","print('---------------------------------------')\n","print('Results for beam 10 prediction: ')\n","print('Candidate:', caption_beam_10)\n","run_all_metrics(reference, caption_beam_10)"]},{"cell_type":"code","execution_count":null,"id":"d2ccce74","metadata":{"id":"d2ccce74"},"outputs":[],"source":["# evaluate the model on all test dataset\n","# prepare test set\n","tokenized_test_descriptions = add_tokens(formatted_descriptions, test)\n","# evaluate model\n","evaluate_model(model, tokenized_test_descriptions, encoding_test, max_length)"]},{"cell_type":"code","execution_count":null,"id":"0230060f","metadata":{"id":"0230060f"},"outputs":[],"source":["# Get predictions for test set and calculate cosine similarity\n","%%time\n","\n","(predictions, references) = get_predictions(model, tokenized_test_descriptions, encoding_test, max_length)\n","\n","cosine_similarity_score = get_cosine_similarity(transformer_model, predictions, references)"]},{"cell_type":"code","source":["cosine_similarity_score"],"metadata":{"id":"9Ih6R5FaY0KN"},"id":"9Ih6R5FaY0KN","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Project_Image_Captions_Final_8k.ipynb","provenance":[{"file_id":"1LeqACAE9n0QXM8Lo40Vn8VqlOR5e8805","timestamp":1659919200967},{"file_id":"1x0F1VjlNCGyylEG_jC1p_0hArZCI5GfQ","timestamp":1659450128757}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":5}